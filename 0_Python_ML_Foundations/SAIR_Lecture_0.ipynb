{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAIR Lecture 0: Python & NumPy Fundamentals for Machine Learning\n",
    "\n",
    "![SAIR Banner](https://raw.githubusercontent.com/silvaxxx1/SAIR/main/SAIR.jpg)\n",
    "\n",
    "**Course:** Practical Introduction to ML/DL Systems  \n",
    "**Instructor:** Mohammed Awad Ahmed (Silva)  \n",
    "**Community:** [SAIR Telegram](https://t.me/+jPPlO6ZFDbtlYzU0)  \n",
    "**License:** MIT\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Master in This Lecture\n",
    "\n",
    "This is your **foundation** for everything that follows in machine learning. Master these concepts, and you'll excel in all subsequent SAIR courses.\n",
    "\n",
    "### Core Learning Objectives\n",
    "\n",
    "‚úÖ **Python Programming** - Variables, functions, loops, OOP  \n",
    "‚úÖ **NumPy Arrays** - The fundamental data structure for ML  \n",
    "‚úÖ **Vectorization** - Writing efficient, fast ML code  \n",
    "‚úÖ **Data Visualization** - Understanding your data and models  \n",
    "‚úÖ **Mathematical Operations** - Linear algebra for ML\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Every single ML algorithm you'll implement in SAIR courses uses these tools:\n",
    "\n",
    "- **Linear Regression** (Lecture 3): NumPy arrays for features and weights\n",
    "- **Neural Networks** (Lecture 6): Vectorized forward/backward propagation\n",
    "- **Computer Vision** (Cluster 6): Image processing with NumPy\n",
    "- **NLP** (Cluster 5): Text preprocessing and embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Quick Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports - RUN THIS FIRST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Professional styling for visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ SAIR Environment Ready!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"All systems go for machine learning! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Python Fundamentals Review\n",
    "\n",
    "## 1.1 Variables and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data types\n",
    "integer_var = 42\n",
    "float_var = 3.14159\n",
    "string_var = \"Machine Learning\"\n",
    "boolean_var = True\n",
    "\n",
    "print(\"üî¢ Basic Data Types:\")\n",
    "print(f\"Integer: {integer_var} (type: {type(integer_var)})\")\n",
    "print(f\"Float: {float_var} (type: {type(float_var)})\")\n",
    "print(f\"String: '{string_var}' (type: {type(string_var)})\")\n",
    "print(f\"Boolean: {boolean_var} (type: {type(boolean_var)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lists and Control Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists and loops\n",
    "features = ['age', 'income', 'education']\n",
    "values = [25, 50000, 16]\n",
    "\n",
    "print(\"üìä Feature Analysis:\")\n",
    "for i, feature in enumerate(features):\n",
    "    print(f\"  Feature {i+1}: {feature} = {values[i]}\")\n",
    "\n",
    "# List comprehensions (Pythonic way)\n",
    "squared_values = [x**2 for x in values]\n",
    "print(f\"\\nSquared values: {squared_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Functions for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction(features, weights, bias):\n",
    "    \"\"\"\n",
    "    Calculate linear model prediction: y = w¬∑x + b\n",
    "    \n",
    "    Args:\n",
    "        features: List of feature values\n",
    "        weights: List of weights\n",
    "        bias: Bias term\n",
    "        \n",
    "    Returns:\n",
    "        Prediction value\n",
    "    \"\"\"\n",
    "    if len(features) != len(weights):\n",
    "        raise ValueError(\"Features and weights must have same length\")\n",
    "    \n",
    "    prediction = sum(f * w for f, w in zip(features, weights)) + bias\n",
    "    return prediction\n",
    "\n",
    "# Test the function\n",
    "test_features = [2, 3, 1]\n",
    "test_weights = [0.5, 0.3, 0.2]\n",
    "test_bias = 1.0\n",
    "\n",
    "pred = calculate_prediction(test_features, test_weights, test_bias)\n",
    "print(f\"üß† Model Prediction: {pred:.2f}\")\n",
    "print(\"This is the foundation of Linear Regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: NumPy - The Engine of Machine Learning\n",
    "\n",
    "## 2.1 Why NumPy for ML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with Python lists for ML\n",
    "python_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "# This doesn't work as expected!\n",
    "list_result = python_list * 2\n",
    "print(\"‚ùå Python list multiplication:\", list_result)\n",
    "\n",
    "# NumPy solution\n",
    "numpy_array = np.array([1, 2, 3, 4, 5])\n",
    "array_result = numpy_array * 2\n",
    "print(\"‚úÖ NumPy array multiplication:\", array_result)\n",
    "\n",
    "print(\"\\nüí° Insight: NumPy enables element-wise operations - essential for ML!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Creating NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Creating NumPy Arrays for ML:\")\n",
    "\n",
    "# 1D array (vector)\n",
    "weights = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "print(f\"Weights vector: {weights}\")\n",
    "print(f\"Shape: {weights.shape}, Dimensions: {weights.ndim}\")\n",
    "\n",
    "# 2D array (matrix) - Most common in ML!\n",
    "feature_matrix = np.array([\n",
    "    [1, 2, 3],  # Sample 1\n",
    "    [4, 5, 6],  # Sample 2\n",
    "    [7, 8, 9]   # Sample 3\n",
    "])\n",
    "print(f\"\\nFeature matrix:\\n{feature_matrix}\")\n",
    "print(f\"Shape: {feature_matrix.shape} ‚Üí (samples, features)\")\n",
    "\n",
    "# Special arrays\n",
    "zeros = np.zeros(5)  # For initializing weights\n",
    "ones = np.ones((2, 3))  # For bias terms\n",
    "random_weights = np.random.randn(4)  # Random initialization\n",
    "\n",
    "print(f\"\\nZeros (initialization): {zeros}\")\n",
    "print(f\"Random weights: {random_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Array Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Dataset example\n",
    "dataset = np.array([\n",
    "    [25, 50000, 1, 0],  # [age, income, education_years, label]\n",
    "    [30, 60000, 2, 1],\n",
    "    [35, 70000, 3, 1],\n",
    "    [40, 80000, 4, 0]\n",
    "])\n",
    "\n",
    "print(\"üìä Dataset:\")\n",
    "print(dataset)\n",
    "print(f\"Shape: {dataset.shape}\")\n",
    "\n",
    "# Extract features and labels (common ML operation)\n",
    "X = dataset[:, :-1]  # All rows, all columns except last\n",
    "y = dataset[:, -1]   # All rows, last column\n",
    "\n",
    "print(f\"\\nFeatures (X):\\n{X}\")\n",
    "print(f\"\\nLabels (y): {y}\")\n",
    "\n",
    "# Access specific samples\n",
    "first_sample = X[0]\n",
    "last_feature = X[:, -1]  # Education years for all samples\n",
    "\n",
    "print(f\"\\nFirst sample: {first_sample}\")\n",
    "print(f\"Education years: {last_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Vectorization - ML Performance Superpower\n",
    "\n",
    "## 3.1 The Power of Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large dataset\n",
    "np.random.seed(42)\n",
    "large_dataset = np.random.randn(1000000)  # 1 million samples\n",
    "\n",
    "# Method 1: Slow loop\n",
    "start_time = time.time()\n",
    "result_loop = []\n",
    "for x in large_dataset:\n",
    "    result_loop.append(x * 2 + 1)\n",
    "result_loop = np.array(result_loop)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# Method 2: Fast vectorized\n",
    "start_time = time.time()\n",
    "result_vectorized = large_dataset * 2 + 1\n",
    "vec_time = time.time() - start_time\n",
    "\n",
    "print(\"‚ö° Performance Comparison (1M operations):\")\n",
    "print(f\"Loop time: {loop_time:.4f} seconds\")\n",
    "print(f\"Vectorized time: {vec_time:.4f} seconds\")\n",
    "print(f\"Speedup: {loop_time/vec_time:.1f}x faster!\")\n",
    "print(f\"\\nResults equal: {np.allclose(result_loop, result_vectorized)}\")\n",
    "\n",
    "print(\"\\nüí° Professional Tip: Always prefer vectorized operations in ML!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic operations\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([5, 6, 7, 8])\n",
    "\n",
    "print(\"üßÆ Vectorized Mathematical Operations:\")\n",
    "print(f\"a + b = {a + b}\")  # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")  # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")  # Element-wise square\n",
    "print(f\"sin(a) = {np.sin(a)}\")  # Trigonometric functions\n",
    "print(f\"exp(a) = {np.exp(a)}\")  # Exponential\n",
    "\n",
    "# Aggregation operations (essential for ML)\n",
    "print(f\"\\nüìà Statistical Operations:\")\n",
    "print(f\"Mean: {np.mean(a):.2f}\")\n",
    "print(f\"Standard deviation: {np.std(a):.2f}\")\n",
    "print(f\"Sum: {np.sum(a)}\")\n",
    "print(f\"Max: {np.max(a)}, Min: {np.min(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Dot Product and Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product - THE most important operation in ML!\n",
    "features = np.array([1.5, 2.0, 0.5])\n",
    "weights = np.array([0.2, 0.3, 0.1])\n",
    "bias = 0.5\n",
    "\n",
    "# Method 1: Manual calculation\n",
    "manual_pred = sum(f * w for f, w in zip(features, weights)) + bias\n",
    "\n",
    "# Method 2: NumPy dot product (preferred)\n",
    "dot_pred = np.dot(features, weights) + bias\n",
    "\n",
    "# Method 3: @ operator (most elegant)\n",
    "at_pred = features @ weights + bias\n",
    "\n",
    "print(\"üß† Linear Model Prediction Methods:\")\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(f\"\\nManual: {manual_pred:.2f}\")\n",
    "print(f\"np.dot(): {dot_pred:.2f}\")\n",
    "print(f\"@ operator: {at_pred:.2f}\")\n",
    "print(f\"\\nAll methods equal: {np.allclose([manual_pred, dot_pred, at_pred], at_pred)}\")\n",
    "\n",
    "print(\"\\nüí° This is how Linear Regression makes predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting example: Feature normalization\n",
    "data = np.array([\n",
    "    [10, 100, 1000],\n",
    "    [20, 200, 2000],\n",
    "    [30, 300, 3000]\n",
    "])\n",
    "\n",
    "print(\"üìä Original Data:\")\n",
    "print(data)\n",
    "\n",
    "# Z-score normalization using broadcasting\n",
    "mean = np.mean(data, axis=0)  # Mean of each column\n",
    "std = np.std(data, axis=0)    # Std of each column\n",
    "\n",
    "normalized_data = (data - mean) / std\n",
    "\n",
    "print(f\"\\nMean per feature: {mean}\")\n",
    "print(f\"Std per feature: {std}\")\n",
    "print(f\"\\nNormalized Data (z-score):\\n{normalized_data}\")\n",
    "print(f\"\\nNew means: {np.mean(normalized_data, axis=0)}\")\n",
    "print(f\"New stds: {np.std(normalized_data, axis=0)}\")\n",
    "\n",
    "print(\"\\nüí° Broadcasting automatically aligns arrays of different shapes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Data Visualization for ML\n",
    "\n",
    "## 4.1 Basic Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Professional plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='sin(x)')\n",
    "plt.title('Sine Wave - Example of ML Feature', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Input Feature (x)', fontsize=12)\n",
    "plt.ylabel('Output (sin(x))', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Visualization is crucial for understanding ML models and data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Scatter Plots for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Class 0\n",
    "class0_x = np.random.normal(2, 1, 50)\n",
    "class0_y = np.random.normal(2, 1, 50)\n",
    "\n",
    "# Class 1\n",
    "class1_x = np.random.normal(6, 1, 50)\n",
    "class1_y = np.random.normal(6, 1, 50)\n",
    "\n",
    "# Create professional scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(class0_x, class0_y, c='red', alpha=0.7, s=60, \n",
    "           edgecolors='white', linewidth=0.5, label='Class 0')\n",
    "plt.scatter(class1_x, class1_y, c='blue', alpha=0.7, s=60,\n",
    "           edgecolors='white', linewidth=0.5, label='Class 1')\n",
    "\n",
    "plt.title('Binary Classification Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ This is what classification data looks like!\")\n",
    "print(\"We'll learn to build models that separate these classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Training Curves (Essential for ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training process\n",
    "epochs = np.arange(1, 51)\n",
    "train_loss = 2.0 * np.exp(-0.1 * epochs) + 0.1 + np.random.normal(0, 0.02, 50)\n",
    "val_loss = 2.0 * np.exp(-0.08 * epochs) + 0.15 + np.random.normal(0, 0.03, 50)\n",
    "\n",
    "# Professional training curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_loss, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "plt.plot(epochs, val_loss, 'r-', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "plt.fill_between(epochs, train_loss, val_loss, alpha=0.2, color='gray')\n",
    "\n",
    "plt.title('Model Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "plt.show()\n",
    "\n",
    "print(\"üìâ Training curves help diagnose model behavior:\")\n",
    "print(\"‚úÖ Good: Both losses decreasing together\")\n",
    "print(\"‚ùå Overfitting: Training loss ‚Üì but validation loss ‚Üë\")\n",
    "print(\"‚ùå Underfitting: Both losses plateau at high values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Object-Oriented Programming for ML\n",
    "\n",
    "## 5.1 ML Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A simple linear model for regression\n",
    "    \n",
    "    This demonstrates how ML models are structured as classes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize model parameters\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(input_dim) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions: y = X @ w + b\"\"\"\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        \"\"\"Calculate Mean Squared Error\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean((predictions - y) ** 2)\n",
    "    \n",
    "    def update_parameters(self, X, y):\n",
    "        \"\"\"Update weights and bias using gradient descent\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        errors = predictions - y\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_weights = (2 / len(y)) * (X.T @ errors)\n",
    "        grad_bias = (2 / len(y)) * np.sum(errors)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.weights -= self.learning_rate * grad_weights\n",
    "        self.bias -= self.learning_rate * grad_bias\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Display model summary\"\"\"\n",
    "        print(f\"Linear Model Summary:\")\n",
    "        print(f\"  Input dimension: {len(self.weights)}\")\n",
    "        print(f\"  Weights: {self.weights}\")\n",
    "        print(f\"  Bias: {self.bias:.4f}\")\n",
    "        print(f\"  Learning rate: {self.learning_rate}\")\n",
    "\n",
    "# Test the model\n",
    "model = LinearModel(input_dim=3, learning_rate=0.01)\n",
    "model.summary()\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "predictions = model.predict(X_test)\n",
    "print(f\"\\nPredictions for test data: {predictions}\")\n",
    "\n",
    "print(\"\\nüí° This class structure is the foundation for all ML models in SAIR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Practical ML Exercises\n",
    "\n",
    "## Exercise 1: Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(X):\n",
    "    \"\"\"\n",
    "    Preprocess dataset for ML training\n",
    "    \n",
    "    Steps:\n",
    "    1. Handle missing values (fill with mean)\n",
    "    2. Normalize features (z-score)\n",
    "    3. Return processed data and parameters\n",
    "    \n",
    "    Args:\n",
    "        X: Input feature matrix\n",
    "        \n",
    "    Returns:\n",
    "        X_processed: Preprocessed data\n",
    "        params: Dictionary of preprocessing parameters\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Step 1: Handle missing values\n",
    "    # Step 2: Normalize features\n",
    "    # Step 3: Return processed data and parameters\n",
    "    \n",
    "    raise NotImplementedError(\"Complete this function!\")\n",
    "\n",
    "# Test data\n",
    "test_X = np.array([\n",
    "    [1, 10, 100],\n",
    "    [2, 20, 200],\n",
    "    [3, 30, 300],\n",
    "    [4, 40, 400]\n",
    "])\n",
    "\n",
    "# TODO: Implement the function above\n",
    "print(\"üöÄ Exercise: Implement the preprocessing pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Mean Squared Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error - the most common regression loss\n",
    "    \n",
    "    Formula: MSE = mean((y_true - y_pred)¬≤)\n",
    "    \n",
    "    Args:\n",
    "        y_true: True target values\n",
    "        y_pred: Predicted values\n",
    "        \n",
    "    Returns:\n",
    "        mse: Mean squared error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Calculate the squared differences\n",
    "    # Return the mean\n",
    "    \n",
    "    raise NotImplementedError(\"Complete this function!\")\n",
    "\n",
    "# Test case\n",
    "y_true = np.array([1, 2, 3, 4, 5])\n",
    "y_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])\n",
    "\n",
    "# TODO: Implement the function\n",
    "print(\"üéØ Exercise: Implement MSE loss function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Congratulations on Completing SAIR Lecture 0!\n",
    "\n",
    "## üèÜ What You've Mastered\n",
    "\n",
    "‚úÖ **Python Fundamentals** - Variables, functions, control structures  \n",
    "‚úÖ **NumPy Arrays** - Creating, indexing, mathematical operations  \n",
    "‚úÖ **Vectorization** - Writing efficient ML code  \n",
    "‚úÖ **Data Visualization** - Understanding data and model behavior  \n",
    "‚úÖ **OOP for ML** - Structured model implementation  \n",
    "‚úÖ **Practical Exercises** - Hands-on ML implementations\n",
    "\n",
    "## üöÄ Next Steps in Your SAIR Journey\n",
    "\n",
    "1. **Complete the exercises** above\n",
    "2. **Join our community** on [Telegram](https://t.me/+jPPlO6ZFDbtlYzU0)\n",
    "3. **Move to Lecture 1** - Linear Regression implementation\n",
    "4. **Build your portfolio** with SAIR projects\n",
    "\n",
    "## üìö Essential Resources\n",
    "\n",
    "- [NumPy Documentation](https://numpy.org/doc/)\n",
    "- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)\n",
    "- [SAIR GitHub Repository](https://github.com/silvaxxx1/SAIR)\n",
    "\n",
    "---\n",
    "\n",
    "**\"The expert in anything was once a beginner.\"**  \n\n",
    "*Keep learning, keep building!* üöÄ\n",
    "\n",
    "**Mohammed Awad Ahmed (Silva)**  \n",
    "*SAIR Founder & Instructor*  \n",
    "*Empowering Sudan through AI Education*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}