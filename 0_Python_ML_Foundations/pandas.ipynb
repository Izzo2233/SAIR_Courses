{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¼ Pandas Mastery: The Complete Data Analysis Guide\n",
    "\n",
    "**SAIR - Sudanese Artificial Intelligence Research**  \n",
    "*From Zero to Pandas Expert for Data Science & Machine Learning*  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- âœ… **Master DataFrames & Series** - The core Pandas data structures\n",
    "- âœ… **Handle real-world data** - Loading, cleaning, and preprocessing\n",
    "- âœ… **Perform advanced analysis** - Grouping, aggregations, and transformations\n",
    "- âœ… **Merge and reshape data** - Joins, pivots, and data restructuring\n",
    "- âœ… **Work with time series** - Date/time operations and resampling\n",
    "- âœ… **Optimize performance** - Efficient data processing techniques\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Why Pandas is ESSENTIAL for Data Science\n",
    "\n",
    "```\n",
    "Data Processing Comparison:\n",
    "Raw Python: Complex loops, error-prone\n",
    "Pandas: Clean, readable, optimized operations\n",
    "```\n",
    "\n",
    "**Every Data Scientist uses Pandas:**\n",
    "- Data cleaning and preprocessing\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Feature engineering for ML\n",
    "- Time series analysis\n",
    "- Database-like operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional setup - RUN THIS FIRST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration for professional output\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"ðŸ”¥ Pandas Professional Environment Ready!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ðŸ—ï¸ DataFrame & Series - The Core Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š 1.1 CREATING DATAFRAMES & SERIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Method 1: From dictionary (most common)\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'],\n",
    "    'Salary': [50000, 60000, 70000, 55000, 65000],\n",
    "    'Department': ['IT', 'HR', 'IT', 'Finance', 'HR']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Index: {df.index.tolist()}\")\n",
    "\n",
    "# Method 2: From list of lists\n",
    "data_list = [\n",
    "    ['Alice', 25, 'New York', 50000, 'IT'],\n",
    "    ['Bob', 30, 'London', 60000, 'HR'],\n",
    "    ['Charlie', 35, 'Tokyo', 70000, 'IT']\n",
    "]\n",
    "columns = ['Name', 'Age', 'City', 'Salary', 'Department']\n",
    "df_list = pd.DataFrame(data_list, columns=columns)\n",
    "print(f\"\\nDataFrame from list of lists:\")\n",
    "print(df_list)\n",
    "\n",
    "# Series - 1D labeled array\n",
    "ages = pd.Series([25, 30, 35, 28, 32], name='Age')\n",
    "print(f\"\\nðŸ“ˆ Series example:\")\n",
    "print(ages)\n",
    "print(f\"Series name: {ages.name}\")\n",
    "print(f\"Series dtype: {ages.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ” 1.2 DATAFRAME PROPERTIES & METADATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a more realistic dataset\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2023-01-01', periods=100, freq='D')\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Product': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'Sales': np.random.randint(100, 1000, 100),\n",
    "    'Customers': np.random.randint(10, 100, 100),\n",
    "    'Revenue': np.random.uniform(1000, 50000, 100)\n",
    "})\n",
    "\n",
    "print(\"Sales Dataset:\")\n",
    "print(sales_data.head(8))\n",
    "\n",
    "print(f\"\\nðŸ“Š DataFrame Properties:\")\n",
    "print(f\"Shape: {sales_data.shape} â†’ (rows, columns)\")\n",
    "print(f\"Columns: {sales_data.columns.tolist()}\")\n",
    "print(f\"Index: {sales_data.index.tolist()[:5]}...\")\n",
    "print(f\"Data types:\\n{sales_data.dtypes}\")\n",
    "print(f\"\\nMemory usage: {sales_data.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Statistical Summary:\")\n",
    "print(sales_data.describe())\n",
    "\n",
    "print(f\"\\nðŸ” Missing Values:\")\n",
    "print(sales_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ðŸŽ¯ Data Selection & Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ 2.1 COLUMN & ROW SELECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Column selection\n",
    "print(\"ðŸ“Š Column Operations:\")\n",
    "print(f\"Single column (Series):\\n{sales_data['Product'].head()}\")\n",
    "print(f\"\\nMultiple columns (DataFrame):\\n{sales_data[['Product', 'Sales', 'Revenue']].head()}\")\n",
    "\n",
    "# Row selection\n",
    "print(f\"\\nðŸ“ Row Operations:\")\n",
    "print(f\"First 5 rows:\\n{sales_data.head()}\")\n",
    "print(f\"\\nLast 3 rows:\\n{sales_data.tail(3)}\")\n",
    "print(f\"\\nRows 10-15:\\n{sales_data.iloc[10:15]}\")\n",
    "\n",
    "# Boolean indexing\n",
    "print(f\"\\nðŸŽ¯ Boolean Indexing:\")\n",
    "high_sales = sales_data[sales_data['Sales'] > 800]\n",
    "print(f\"High sales (>800): {len(high_sales)} records\")\n",
    "print(high_sales.head())\n",
    "\n",
    "# Multiple conditions\n",
    "north_high_sales = sales_data[(sales_data['Region'] == 'North') & (sales_data['Sales'] > 700)]\n",
    "print(f\"\\nNorth region high sales: {len(north_high_sales)} records\")\n",
    "print(north_high_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¥ 2.2 LOC & ILOC - PRECISE DATA ACCESS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# loc - label-based indexing\n",
    "print(\"ðŸ“ loc (label-based):\")\n",
    "print(\"Rows 5-7, specific columns:\")\n",
    "print(sales_data.loc[5:7, ['Product', 'Region', 'Sales']])\n",
    "\n",
    "# iloc - position-based indexing\n",
    "print(f\"\\nðŸ”¢ iloc (position-based):\")\n",
    "print(\"Rows 0, 2, 4 and columns 1, 3, 4:\")\n",
    "print(sales_data.iloc[[0, 2, 4], [1, 3, 4]])\n",
    "\n",
    "# Conditional selection with loc\n",
    "print(f\"\\nðŸŽ¯ Conditional loc:\")\n",
    "product_a_high_revenue = sales_data.loc[(sales_data['Product'] == 'A') & (sales_data['Revenue'] > 25000)]\n",
    "print(f\"Product A with revenue > 25k: {len(product_a_high_revenue)} records\")\n",
    "print(product_a_high_revenue.head())\n",
    "\n",
    "# Setting values with loc\n",
    "sales_data_copy = sales_data.copy()\n",
    "sales_data_copy.loc[sales_data_copy['Sales'] < 200, 'Sales_Status'] = 'Low'\n",
    "sales_data_copy.loc[sales_data_copy['Sales'] >= 200, 'Sales_Status'] = 'High'\n",
    "print(f\"\\nðŸ”„ Added Sales_Status column:\")\n",
    "print(sales_data_copy[['Sales', 'Sales_Status']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ðŸ§¹ Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ§¹ 3.1 HANDLING MISSING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create dataset with missing values\n",
    "data_with_nan = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', None, 'Eve'],\n",
    "    'Age': [25, np.nan, 35, 28, None],\n",
    "    'Salary': [50000, 60000, None, 55000, 65000],\n",
    "    'Department': ['IT', 'HR', 'IT', None, 'HR']\n",
    "})\n",
    "\n",
    "print(\"Dataset with missing values:\")\n",
    "print(data_with_nan)\n",
    "print(f\"\\nðŸ“Š Missing values summary:\")\n",
    "print(data_with_nan.isnull().sum())\n",
    "\n",
    "# Handling missing values\n",
    "print(f\"\\nðŸ”„ Missing Value Treatment:\")\n",
    "\n",
    "# Method 1: Fill with specific values\n",
    "filled_data = data_with_nan.fillna({\n",
    "    'Name': 'Unknown',\n",
    "    'Age': data_with_nan['Age'].mean(),\n",
    "    'Salary': data_with_nan['Salary'].median(),\n",
    "    'Department': 'Unknown'\n",
    "})\n",
    "print(\"After filling missing values:\")\n",
    "print(filled_data)\n",
    "\n",
    "# Method 2: Drop missing values\n",
    "dropped_data = data_with_nan.dropna()\n",
    "print(f\"\\nAfter dropping rows with missing values: {len(dropped_data)} records remaining\")\n",
    "print(dropped_data)\n",
    "\n",
    "# Method 3: Forward/backward fill\n",
    "time_series_data = pd.DataFrame({\n",
    "    'Date': pd.date_range('2023-01-01', periods=10),\n",
    "    'Value': [1, np.nan, np.nan, 4, 5, np.nan, 7, 8, np.nan, 10]\n",
    "})\n",
    "time_series_data['Forward_Fill'] = time_series_data['Value'].ffill()\n",
    "time_series_data['Backward_Fill'] = time_series_data['Value'].bfill()\n",
    "print(f\"\\nðŸ“ˆ Time series with forward/backward fill:\")\n",
    "print(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”§ 3.2 DATA TYPE CONVERSION & CLEANING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create messy data\n",
    "messy_data = pd.DataFrame({\n",
    "    'ID': ['001', '002', '003', '004', '005'],\n",
    "    'Price': ['$100.50', '$200.75', '$150.25', '$300.00', '$250.50'],\n",
    "    'Quantity': ['10', '15', '8', '20', '12'],\n",
    "    'Category': ['Electronics ', '  Books', 'Electronics', 'Books ', 'ELECTRONICS'],\n",
    "    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']\n",
    "})\n",
    "\n",
    "print(\"Original messy data:\")\n",
    "print(messy_data)\n",
    "print(f\"\\nData types:\\n{messy_data.dtypes}\")\n",
    "\n",
    "# Data cleaning pipeline\n",
    "cleaned_data = messy_data.copy()\n",
    "\n",
    "# 1. Clean strings and convert data types\n",
    "cleaned_data['Price'] = cleaned_data['Price'].str.replace('$', '').astype(float)\n",
    "cleaned_data['Quantity'] = cleaned_data['Quantity'].astype(int)\n",
    "cleaned_data['Category'] = cleaned_data['Category'].str.strip().str.title()\n",
    "cleaned_data['Date'] = pd.to_datetime(cleaned_data['Date'])\n",
    "\n",
    "# 2. Add calculated column\n",
    "cleaned_data['Total_Value'] = cleaned_data['Price'] * cleaned_data['Quantity']\n",
    "\n",
    "print(f\"\\nâœ… After cleaning and transformation:\")\n",
    "print(cleaned_data)\n",
    "print(f\"\\nCleaned data types:\\n{cleaned_data.dtypes}\")\n",
    "\n",
    "# String operations\n",
    "print(f\"\\nðŸ“ String Operations:\")\n",
    "cleaned_data['Category_Length'] = cleaned_data['Category'].str.len()\n",
    "cleaned_data['Category_Upper'] = cleaned_data['Category'].str.upper()\n",
    "print(cleaned_data[['Category', 'Category_Length', 'Category_Upper']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ðŸ“Š Data Analysis & Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ˆ 4.1 GROUPBY OPERATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Basic groupby\n",
    "print(\"ðŸ“Š Sales by Product:\")\n",
    "product_sales = sales_data.groupby('Product')['Sales'].agg(['sum', 'mean', 'count', 'std'])\n",
    "print(product_sales)\n",
    "\n",
    "# Multiple aggregations\n",
    "print(f\"\\nðŸŽ¯ Sales by Region and Product:\")\n",
    "region_product_stats = sales_data.groupby(['Region', 'Product']).agg({\n",
    "    'Sales': ['sum', 'mean', 'max', 'min'],\n",
    "    'Revenue': ['sum', 'mean'],\n",
    "    'Customers': 'count'\n",
    "}).round(2)\n",
    "print(region_product_stats)\n",
    "\n",
    "# Named aggregations (cleaner output)\n",
    "print(f\"\\nðŸ·ï¸ Named Aggregations:\")\n",
    "named_agg = sales_data.groupby('Region').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    avg_revenue=('Revenue', 'mean'),\n",
    "    max_customers=('Customers', 'max'),\n",
    "    transaction_count=('Sales', 'count')\n",
    ").round(2)\n",
    "print(named_agg)\n",
    "\n",
    "# Transform - add group statistics to original data\n",
    "sales_data['Region_Avg_Sales'] = sales_data.groupby('Region')['Sales'].transform('mean')\n",
    "sales_data['Sales_vs_Region_Avg'] = sales_data['Sales'] - sales_data['Region_Avg_Sales']\n",
    "print(f\"\\nðŸ”„ Added group statistics:\")\n",
    "print(sales_data[['Region', 'Sales', 'Region_Avg_Sales', 'Sales_vs_Region_Avg']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š 4.2 PIVOT TABLES & CROSSTABS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Pivot table\n",
    "print(\"ðŸ“ˆ Pivot Table - Average Sales by Region and Product:\")\n",
    "pivot_sales = sales_data.pivot_table(\n",
    "    values='Sales',\n",
    "    index='Region',\n",
    "    columns='Product',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ").round(2)\n",
    "print(pivot_sales)\n",
    "\n",
    "# Multi-level pivot\n",
    "print(f\"\\nðŸŽ¯ Multi-level Pivot - Sales and Revenue:\")\n",
    "multi_pivot = sales_data.pivot_table(\n",
    "    values=['Sales', 'Revenue'],\n",
    "    index='Region',\n",
    "    columns='Product',\n",
    "    aggfunc={'Sales': 'mean', 'Revenue': 'sum'}\n",
    ").round(2)\n",
    "print(multi_pivot)\n",
    "\n",
    "# Crosstab - frequency table\n",
    "print(f\"\\nðŸ“Š Crosstab - Region vs Product Counts:\")\n",
    "cross_tab = pd.crosstab(\n",
    "    sales_data['Region'],\n",
    "    sales_data['Product'],\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "print(cross_tab)\n",
    "\n",
    "# Normalized crosstab\n",
    "print(f\"\\nðŸ“ˆ Normalized Crosstab (row percentages):\")\n",
    "cross_tab_norm = pd.crosstab(\n",
    "    sales_data['Region'],\n",
    "    sales_data['Product'],\n",
    "    normalize='index'\n",
    ").round(4)\n",
    "print(cross_tab_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ðŸ”„ Data Merging & Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ 5.1 MERGING DATAFRAMES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample datasets\n",
    "employees = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'DepartmentID': [101, 102, 101, 103, 102]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'DepartmentID': [101, 102, 103, 104],\n",
    "    'DepartmentName': ['IT', 'HR', 'Finance', 'Marketing'],\n",
    "    'Manager': ['John', 'Sarah', 'Mike', 'Lisa']\n",
    "})\n",
    "\n",
    "salaries = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4, 6],  # Note: Employee 6 doesn't exist in employees\n",
    "    'Salary': [50000, 60000, 70000, 55000, 65000]\n",
    "})\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "print(employees)\n",
    "print(f\"\\nDepartments DataFrame:\")\n",
    "print(departments)\n",
    "print(f\"\\nSalaries DataFrame:\")\n",
    "print(salaries)\n",
    "\n",
    "# Inner join\n",
    "emp_dept_inner = pd.merge(employees, departments, on='DepartmentID', how='inner')\n",
    "print(f\"\\nâœ… Inner Join (employees Ã— departments):\")\n",
    "print(emp_dept_inner)\n",
    "\n",
    "# Left join\n",
    "emp_salaries_left = pd.merge(employees, salaries, on='EmployeeID', how='left')\n",
    "print(f\"\\nðŸ”µ Left Join (all employees with salaries):\")\n",
    "print(emp_salaries_left)\n",
    "\n",
    "# Outer join\n",
    "emp_salaries_outer = pd.merge(employees, salaries, on='EmployeeID', how='outer', indicator=True)\n",
    "print(f\"\\nðŸŸ¢ Outer Join (all records from both):\")\n",
    "print(emp_salaries_outer)\n",
    "\n",
    "# Multiple joins\n",
    "final_data = pd.merge(emp_dept_inner, salaries, on='EmployeeID', how='left')\n",
    "print(f\"\\nðŸŽ¯ Final Combined Dataset:\")\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š 5.2 CONCATENATION & APPENDING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample quarterly data\n",
    "q1_sales = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C'],\n",
    "    'Q1_Sales': [100, 150, 200]\n",
    "})\n",
    "\n",
    "q2_sales = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'D'],  # Note: Different products\n",
    "    'Q2_Sales': [120, 160, 180]\n",
    "})\n",
    "\n",
    "q3_sales = pd.DataFrame({\n",
    "    'Product': ['A', 'C', 'E'],\n",
    "    'Q3_Sales': [130, 210, 190]\n",
    "})\n",
    "\n",
    "print(\"Quarterly Sales Data:\")\n",
    "print(\"Q1:\")\n",
    "print(q1_sales)\n",
    "print(\"\\nQ2:\")\n",
    "print(q2_sales)\n",
    "print(\"\\nQ3:\")\n",
    "print(q3_sales)\n",
    "\n",
    "# Concatenate horizontally\n",
    "quarterly_combined = pd.concat([q1_sales, q2_sales, q3_sales], axis=1)\n",
    "print(f\"\\nðŸ“ˆ Horizontal Concatenation:\")\n",
    "print(quarterly_combined)\n",
    "\n",
    "# Merge with outer join to handle all products\n",
    "all_quarters = q1_sales.merge(q2_sales, on='Product', how='outer').merge(q3_sales, on='Product', how='outer')\n",
    "print(f\"\\nðŸ”„ Outer Merge (all products):\")\n",
    "print(all_quarters)\n",
    "\n",
    "# Melt - wide to long format\n",
    "melted_data = pd.melt(\n",
    "    all_quarters,\n",
    "    id_vars=['Product'],\n",
    "    value_vars=['Q1_Sales', 'Q2_Sales', 'Q3_Sales'],\n",
    "    var_name='Quarter',\n",
    "    value_name='Sales'\n",
    ")\n",
    "print(f\"\\nðŸ“Š Melted Data (long format):\")\n",
    "print(melted_data.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. â° Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâ° 6.1 DATE/TIME OPERATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create time series data\n",
    "dates = pd.date_range('2023-01-01', periods=365, freq='D')\n",
    "time_series = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Sales': np.random.randint(100, 1000, 365) + np.sin(np.arange(365) * 0.1) * 100,\n",
    "    'Temperature': np.random.randint(10, 35, 365) + np.cos(np.arange(365) * 0.05) * 10\n",
    "})\n",
    "\n",
    "print(\"Time Series Data:\")\n",
    "print(time_series.head(10))\n",
    "print(f\"\\nDate range: {time_series['Date'].min()} to {time_series['Date'].max()}\")\n",
    "\n",
    "# Date extraction\n",
    "time_series['Year'] = time_series['Date'].dt.year\n",
    "time_series['Month'] = time_series['Date'].dt.month\n",
    "time_series['Day'] = time_series['Date'].dt.day\n",
    "time_series['DayOfWeek'] = time_series['Date'].dt.day_name()\n",
    "time_series['Quarter'] = time_series['Date'].dt.quarter\n",
    "\n",
    "print(f\"\\nðŸ“… With Date Components:\")\n",
    "print(time_series.head())\n",
    "\n",
    "# Filter by date\n",
    "q1_2023 = time_series[time_series['Date'].between('2023-01-01', '2023-03-31')]\n",
    "print(f\"\\nðŸ“Š Q1 2023 Data ({len(q1_2023)} records):\")\n",
    "print(q1_2023.head())\n",
    "\n",
    "# Set date as index\n",
    "time_series_indexed = time_series.set_index('Date')\n",
    "print(f\"\\nðŸŽ¯ With Date as Index:\")\n",
    "print(time_series_indexed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ˆ 6.2 RESAMPLING & ROLLING OPERATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Resampling - change frequency\n",
    "monthly_sales = time_series_indexed['Sales'].resample('M').agg(['sum', 'mean', 'std'])\n",
    "print(\"ðŸ“… Monthly Sales Aggregation:\")\n",
    "print(monthly_sales.round(2))\n",
    "\n",
    "# Weekly averages\n",
    "weekly_avg = time_series_indexed['Sales'].resample('W').mean()\n",
    "print(f\"\\nðŸ“Š Weekly Average Sales:\")\n",
    "print(weekly_avg.head(10))\n",
    "\n",
    "# Rolling statistics\n",
    "time_series_indexed['Sales_7D_Avg'] = time_series_indexed['Sales'].rolling(window=7).mean()\n",
    "time_series_indexed['Sales_30D_Avg'] = time_series_indexed['Sales'].rolling(window=30).mean()\n",
    "time_series_indexed['Sales_7D_Std'] = time_series_indexed['Sales'].rolling(window=7).std()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Rolling Statistics (first 30 days):\")\n",
    "print(time_series_indexed[['Sales', 'Sales_7D_Avg', 'Sales_30D_Avg', 'Sales_7D_Std']].head(30).round(2))\n",
    "\n",
    "# Expanding window\n",
    "time_series_indexed['Sales_Expanding_Mean'] = time_series_indexed['Sales'].expanding().mean()\n",
    "print(f\"\\nðŸ“Š Expanding Mean (first 10 days):\")\n",
    "print(time_series_indexed[['Sales', 'Sales_Expanding_Mean']].head(10).round(2))\n",
    "\n",
    "# Plotting time series\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "time_series_indexed['Sales'].plot(title='Daily Sales', color='blue')\n",
    "plt.ylabel('Sales')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "monthly_sales['mean'].plot(kind='bar', title='Monthly Average Sales', color='green')\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "time_series_indexed['Sales_7D_Avg'].plot(title='7-Day Moving Average', color='red')\n",
    "plt.ylabel('Sales')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "time_series_indexed['Sales_Expanding_Mean'].plot(title='Expanding Mean', color='purple')\n",
    "plt.ylabel('Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. ðŸš€ Advanced Operations & Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâš¡ 7.1 VECTORIZED OPERATIONS & APPLY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Vectorized operations (fast!)\n",
    "sales_data['Sales_Squared'] = sales_data['Sales'] ** 2\n",
    "sales_data['Revenue_Per_Customer'] = sales_data['Revenue'] / sales_data['Customers']\n",
    "sales_data['Sales_Category'] = np.where(sales_data['Sales'] > 500, 'High', 'Low')\n",
    "\n",
    "print(\"ðŸ“Š With Vectorized Operations:\")\n",
    "print(sales_data[['Sales', 'Sales_Squared', 'Revenue_Per_Customer', 'Sales_Category']].head())\n",
    "\n",
    "# Apply function to Series\n",
    "def categorize_revenue(revenue):\n",
    "    if revenue < 10000:\n",
    "        return 'Low'\n",
    "    elif revenue < 25000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "sales_data['Revenue_Category'] = sales_data['Revenue'].apply(categorize_revenue)\n",
    "print(f\"\\nðŸŽ¯ Revenue Categories:\")\n",
    "print(sales_data[['Revenue', 'Revenue_Category']].head())\n",
    "\n",
    "# Apply function to DataFrame rows\n",
    "def calculate_efficiency(row):\n",
    "    return row['Revenue'] / row['Customers'] if row['Customers'] > 0 else 0\n",
    "\n",
    "sales_data['Efficiency'] = sales_data.apply(calculate_efficiency, axis=1)\n",
    "print(f\"\\nðŸ“ˆ Efficiency Calculation:\")\n",
    "print(sales_data[['Revenue', 'Customers', 'Efficiency']].head().round(2))\n",
    "\n",
    "# Vectorized string operations\n",
    "sales_data['Region_Code'] = sales_data['Region'].str[:3].str.upper()\n",
    "sales_data['Product_Lower'] = sales_data['Product'].str.lower()\n",
    "print(f\"\\nðŸ“ String Operations:\")\n",
    "print(sales_data[['Region', 'Region_Code', 'Product', 'Product_Lower']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ’¾ 7.2 IO OPERATIONS & PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample data for IO operations\n",
    "large_dataset = pd.DataFrame({\n",
    "    'ID': range(1, 10001),\n",
    "    'Value1': np.random.randn(10000),\n",
    "    'Value2': np.random.randint(1, 100, 10000),\n",
    "    'Category': np.random.choice(['A', 'B', 'C', 'D'], 10000),\n",
    "    'Timestamp': pd.date_range('2023-01-01', periods=10000, freq='H')\n",
    "})\n",
    "\n",
    "print(\"Large Dataset Sample:\")\n",
    "print(large_dataset.head())\n",
    "print(f\"\\nDataset size: {len(large_dataset):,} rows\")\n",
    "\n",
    "# Save to different formats\n",
    "print(f\"\\nðŸ’¾ Saving to different formats:\")\n",
    "\n",
    "# CSV\n",
    "large_dataset.to_csv('sample_data.csv', index=False)\n",
    "print(\"âœ… Saved to CSV: sample_data.csv\")\n",
    "\n",
    "# Excel (requires openpyxl)\n",
    "try:\n",
    "    large_dataset.to_excel('sample_data.xlsx', index=False, sheet_name='Data')\n",
    "    print(\"âœ… Saved to Excel: sample_data.xlsx\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Excel export requires openpyxl: pip install openpyxl\")\n",
    "\n",
    "# Parquet (efficient for large datasets)\n",
    "try:\n",
    "    large_dataset.to_parquet('sample_data.parquet', index=False)\n",
    "    print(\"âœ… Saved to Parquet: sample_data.parquet\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Parquet export requires pyarrow: pip install pyarrow\")\n",
    "\n",
    "# Reading data back\n",
    "print(f\"\\nðŸ“– Reading data back:\")\n",
    "df_csv = pd.read_csv('sample_data.csv')\n",
    "print(f\"CSV data shape: {df_csv.shape}\")\n",
    "\n",
    "# Memory optimization\n",
    "print(f\"\\nðŸ’¡ Memory Optimization:\")\n",
    "print(f\"Original memory usage: {large_dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Downcast numeric columns\n",
    "optimized = large_dataset.copy()\n",
    "optimized['Value2'] = pd.to_numeric(optimized['Value2'], downcast='integer')\n",
    "optimized['Value1'] = pd.to_numeric(optimized['Value1'], downcast='float')\n",
    "print(f\"Optimized memory usage: {optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory saved: {(large_dataset.memory_usage(deep=True).sum() - optimized.memory_usage(deep=True).sum()) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ðŸŽ¯ Real-World Data Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸª 8.1 RETAIL SALES ANALYSIS PROJECT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive retail dataset\n",
    "np.random.seed(42)\n",
    "n_records = 1000\n",
    "\n",
    "retail_data = pd.DataFrame({\n",
    "    'TransactionID': range(1, n_records + 1),\n",
    "    'Date': pd.date_range('2023-01-01', periods=n_records, freq='H'),\n",
    "    'Store': np.random.choice(['Store_A', 'Store_B', 'Store_C', 'Store_D'], n_records),\n",
    "    'Product_Category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Sports', 'Books'], n_records),\n",
    "    'Product': np.random.choice(['Laptop', 'Shirt', 'Table', 'Basketball', 'Novel'], n_records),\n",
    "    'Quantity': np.random.randint(1, 5, n_records),\n",
    "    'Unit_Price': np.random.uniform(10, 500, n_records),\n",
    "    'Customer_Type': np.random.choice(['New', 'Returning', 'VIP'], n_records),\n",
    "    'Payment_Method': np.random.choice(['Credit', 'Debit', 'Cash', 'Mobile'], n_records)\n",
    "})\n",
    "\n",
    "# Calculate derived columns\n",
    "retail_data['Total_Amount'] = retail_data['Quantity'] * retail_data['Unit_Price']\n",
    "retail_data['Month'] = retail_data['Date'].dt.month\n",
    "retail_data['DayOfWeek'] = retail_data['Date'].dt.day_name()\n",
    "retail_data['Hour'] = retail_data['Date'].dt.hour\n",
    "\n",
    "print(\"ðŸ›’ Retail Sales Dataset:\")\n",
    "print(retail_data.head(8))\n",
    "print(f\"\\nDataset Info: {len(retail_data):,} transactions\")\n",
    "\n",
    "# Comprehensive analysis\n",
    "print(f\"\\nðŸ“Š BUSINESS ANALYSIS REPORT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 1. Overall performance\n",
    "total_revenue = retail_data['Total_Amount'].sum()\n",
    "avg_transaction = retail_data['Total_Amount'].mean()\n",
    "total_transactions = len(retail_data)\n",
    "\n",
    "print(f\"ðŸ’° Overall Performance:\")\n",
    "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"Average Transaction: ${avg_transaction:.2f}\")\n",
    "print(f\"Total Transactions: {total_transactions:,}\")\n",
    "\n",
    "# 2. Store performance\n",
    "store_performance = retail_data.groupby('Store').agg({\n",
    "    'Total_Amount': ['sum', 'mean', 'count'],\n",
    "    'Quantity': 'sum'\n",
    "}).round(2)\n",
    "store_performance.columns = ['Total_Revenue', 'Avg_Transaction', 'Transaction_Count', 'Total_Quantity']\n",
    "print(f\"\\nðŸª Store Performance:\")\n",
    "print(store_performance)\n",
    "\n",
    "# 3. Category analysis\n",
    "category_analysis = retail_data.groupby('Product_Category').agg({\n",
    "    'Total_Amount': 'sum',\n",
    "    'Quantity': 'sum',\n",
    "    'TransactionID': 'count'\n",
    "    }).round(2)\n",
    "category_analysis.columns = ['Total_Revenue', 'Total_Quantity', 'Transaction_Count']\n",
    "category_analysis['Revenue_Per_Transaction'] = category_analysis['Total_Revenue'] / category_analysis['Transaction_Count']\n",
    "print(f\"\\nðŸ“¦ Category Analysis:\")\n",
    "print(category_analysis.round(2))\n",
    "\n",
    "# 4. Time-based analysis\n",
    "monthly_revenue = retail_data.groupby('Month')['Total_Amount'].sum()\n",
    "daily_pattern = retail_data.groupby('Hour')['Total_Amount'].sum()\n",
    "weekly_pattern = retail_data.groupby('DayOfWeek')['Total_Amount'].sum()\n",
    "\n",
    "print(f\"\\nâ° Time-Based Analysis:\")\n",
    "print(f\"Monthly Revenue:\\n{monthly_revenue.round(2)}\")\n",
    "print(f\"\\nPeak Hours (Top 5):\")\n",
    "print(daily_pattern.sort_values(ascending=False).head())\n",
    "print(f\"\\nWeekly Pattern:\")\n",
    "print(weekly_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ˆ 8.2 ADVANCED VISUALIZATION & INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Revenue by Store\n",
    "plt.subplot(2, 3, 1)\n",
    "store_revenue = retail_data.groupby('Store')['Total_Amount'].sum()\n",
    "store_revenue.plot(kind='bar', color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "plt.title('Total Revenue by Store', fontweight='bold')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Category Distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "category_dist = retail_data['Product_Category'].value_counts()\n",
    "plt.pie(category_dist.values, labels=category_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Product Category Distribution', fontweight='bold')\n",
    "\n",
    "# 3. Monthly Revenue Trend\n",
    "plt.subplot(2, 3, 3)\n",
    "monthly_revenue = retail_data.groupby('Month')['Total_Amount'].sum()\n",
    "monthly_revenue.plot(kind='line', marker='o', color='#FF6B6B')\n",
    "plt.title('Monthly Revenue Trend', fontweight='bold')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Customer Type Analysis\n",
    "plt.subplot(2, 3, 4)\n",
    "customer_analysis = retail_data.groupby('Customer_Type').agg({\n",
    "    'Total_Amount': 'mean',\n",
    "    'TransactionID': 'count'\n",
    "})\n",
    "customer_analysis.columns = ['Avg_Spend', 'Transaction_Count']\n",
    "customer_analysis['Avg_Spend'].plot(kind='bar', color=['#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "plt.title('Average Spend by Customer Type', fontweight='bold')\n",
    "plt.ylabel('Average Spend ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 5. Hourly Sales Pattern\n",
    "plt.subplot(2, 3, 5)\n",
    "hourly_sales = retail_data.groupby('Hour')['Total_Amount'].sum()\n",
    "hourly_sales.plot(kind='area', color='#FFA726', alpha=0.7)\n",
    "plt.title('Hourly Sales Pattern', fontweight='bold')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Payment Method Preference\n",
    "plt.subplot(2, 3, 6)\n",
    "payment_methods = retail_data['Payment_Method'].value_counts()\n",
    "payment_methods.plot(kind='barh', color=['#AB47BC', '#7E57C2', '#5C6BC0', '#42A5F5'])\n",
    "plt.title('Payment Method Preference', fontweight='bold')\n",
    "plt.xlabel('Number of Transactions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key Insights\n",
    "print(\"\\nðŸ’¡ KEY BUSINESS INSIGHTS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Top performing store\n",
    "top_store = store_revenue.idxmax()\n",
    "top_store_revenue = store_revenue.max()\n",
    "print(f\"ðŸ† Top Performing Store: {top_store} (${top_store_revenue:,.2f})\")\n",
    "\n",
    "# Most profitable category\n",
    "top_category = category_analysis['Total_Revenue'].idxmax()\n",
    "top_category_revenue = category_analysis['Total_Revenue'].max()\n",
    "print(f\"ðŸ“¦ Most Profitable Category: {top_category} (${top_category_revenue:,.2f})\")\n",
    "\n",
    "# Best customer type\n",
    "best_customer = customer_analysis['Avg_Spend'].idxmax()\n",
    "best_customer_spend = customer_analysis['Avg_Spend'].max()\n",
    "print(f\"ðŸ‘¥ Highest Spending Customers: {best_customer} (${best_customer_spend:.2f} avg)\")\n",
    "\n",
    "# Peak business hour\n",
    "peak_hour = hourly_sales.idxmax()\n",
    "peak_hour_revenue = hourly_sales.max()\n",
    "print(f\"â° Peak Business Hour: {peak_hour}:00 (${peak_hour_revenue:.2f})\")\n",
    "\n",
    "# Most popular payment method\n",
    "popular_payment = payment_methods.idxmax()\n",
    "print(f\"ðŸ’³ Most Popular Payment: {popular_payment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Pandas Mastery Achievement Unlocked!\n",
    "\n",
    "## ðŸ† What You've Accomplished\n",
    "\n",
    "âœ… **DataFrame & Series Mastery** - Core data structures  \n",
    "âœ… **Data Cleaning Expertise** - Handling real-world messy data  \n",
    "âœ… **Advanced Analysis** - Grouping, aggregations, pivots  \n",
    "âœ… **Data Merging Skills** - Joins, concatenation, reshaping  \n",
    "âœ… **Time Series Analysis** - Date operations and resampling  \n",
    "âœ… **Real-World Project** - Complete business analysis  \n",
    "âœ… **Performance Optimization** - Efficient data processing\n",
    "\n",
    "## ðŸš€ Next Steps in Your Data Science Journey\n",
    "\n",
    "1. **Practice** with the exercises below\n",
    "2. **Explore** real datasets from Kaggle or your domain\n",
    "3. **Learn** data visualization with Seaborn and Plotly\n",
    "4. **Advance** to machine learning with scikit-learn\n",
    "5. **Join** the SAIR community for projects and collaboration\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª Final Comprehensive Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ COMPREHENSIVE EXERCISES - TEST YOUR MASTERY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def exercise_1():\n",
    "    \"\"\"Customer Segmentation Analysis\"\"\"\n",
    "    print(\"\\n1. Customer Segmentation Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Create customer segments based on spending behavior\n",
    "    # - High Value: Total spend > $1000\n",
    "    # - Medium Value: Total spend between $500-$1000  \n",
    "    # - Low Value: Total spend < $500\n",
    "    # Calculate average transaction value and frequency for each segment\n",
    "    \n",
    "    print(\"Implement customer segmentation analysis!\")\n",
    "\n",
    "def exercise_2():\n",
    "    \"\"\"Sales Forecasting Preparation\"\"\"\n",
    "    print(\"\\n2. Sales Forecasting Preparation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Prepare data for time series forecasting\n",
    "    # - Create daily sales aggregates\n",
    "    # - Calculate 7-day and 30-day moving averages\n",
    "    # - Identify seasonal patterns\n",
    "    # - Handle missing dates in the time series\n",
    "    \n",
    "    print(\"Prepare sales data for forecasting models!\")\n",
    "\n",
    "def exercise_3():\n",
    "    \"\"\"Data Quality Assessment\"\"\"\n",
    "    print(\"\\n3. Data Quality Assessment\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Create a comprehensive data quality report\n",
    "    # - Check for duplicates and inconsistencies\n",
    "    # - Validate data ranges and business rules\n",
    "    # - Identify outliers in numerical columns\n",
    "    # - Assess data completeness and accuracy\n",
    "    \n",
    "    print(\"Implement data quality assessment framework!\")\n",
    "\n",
    "# Run exercises\n",
    "exercise_1()\n",
    "exercise_2()\n",
    "exercise_3()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ CONGRATULATIONS! You've completed Pandas Mastery!\")\n",
    "print(\"\\nYou now have the foundation to excel in:\")\n",
    "print(\"âœ… Data Analysis & Business Intelligence\")\n",
    "print(\"âœ… Machine Learning Data Preparation\")\n",
    "print(\"âœ… Time Series Analysis\")\n",
    "print(\"âœ… Database Operations & ETL\")\n",
    "print(\"âœ… Real-World Business Analytics\")\n",
    "print(\"\\nKeep analyzing! ðŸ“ŠðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}